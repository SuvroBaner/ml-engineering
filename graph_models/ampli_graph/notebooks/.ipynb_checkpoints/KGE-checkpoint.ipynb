{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66976cae-76d7-40a3-8bdd-98df58733eca",
   "metadata": {},
   "source": [
    "## Importing Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfb68db9-aa9f-46ea-a457-663d8f7f3b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import ampligraph\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de82c15b-433b-4a35-8990-174008f518d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ampligraph.datasets import load_fb15k_237, load_wn18rr, load_yago3_10\n",
    "from ampligraph.evaluation import train_test_split_no_unseen, evaluate_performance, mr_score, mrr_score, hits_at_n_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3801eb2a-8b76-4c84-a494-3e8ed76dd654",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ampligraph.discovery import query_topn, discover_facts, find_clusters\n",
    "from ampligraph.latent_features import TransE, ComplEx, HolE, DistMult, ConvE, ConvKB\n",
    "from ampligraph.utils import save_model, restore_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "82455256-ab97-4b93-9323-1cb7607907af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ampligraph.evaluation import train_test_split_no_unseen\n",
    "from ampligraph.utils import save_model, restore_model\n",
    "from ampligraph.evaluation import evaluate_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26424381-3dac-46db-8a96-4376110c01d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ampligraph version : 1.4.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Ampligraph version : {}\".format(ampligraph.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d35ec47a-576b-4e96-a4ab-3216d2e93404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 1.15.5\n"
     ]
    }
   ],
   "source": [
    "print(\"Tensorflow version: {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cdad60-c079-479c-b0ee-e3563c0f0496",
   "metadata": {},
   "source": [
    "# Loading the KG (Knowledge Graph) dataset -\n",
    "\n",
    "A standard KG called **Freebase-15k-237** will be loaded. You can load KGs, csvs, ntriples etc from the API : https://docs.ampligraph.org/en/1.4.0/ampligraph.datasets.html\n",
    "\n",
    "* FB15k-237 dataset : Freebase knowledge base (ontology behind Google's semantic search feature (knowledge graph) which is a backend for Google search results that include structured asnwers to querues instead of series of links to external resources.) Its is 1.9 billion triples in the format (rfd - resource description format). Google bought it in 2010.IN 2016 it was closed and was migrated to Wikidata. FB15k-237 is a link prediction dataset created from FB15k. While FB15k consists of 1,345 relations, 14,951 entities, and 592,213 triples, many triples are inverses that cause leakage from the training to testing and validation splits. FB15k-237 was created by Toutanova and Chen (2015) to ensure that the testing and evaluation datasets do not have inverse relation test leakage. In summary, FB15k-237 dataset contains 310,079 triples with 14,505 entities and 237 relation types.\n",
    "\n",
    "https://paperswithcode.com/dataset/fb15k-237\n",
    "\n",
    "* wn18rr dataset : WN18RR is a link prediction dataset created from WN18, which is a subset of WordNet. WN18 consists of 18 relations and 40,943 entities. However, many text triples are obtained by inverting triples from the training set. Thus the WN18RR dataset is created to ensure that the evaluation dataset does not have inverse relation test leakage. In summary, WN18RR dataset contains 93,003 triples with 40,943 entities and 11 relation types.\n",
    "\n",
    "https://paperswithcode.com/dataset/wn18rr\n",
    "\n",
    "* yago3 : YAGO3-10 is benchmark dataset for knowledge base completion. It is a subset of YAGO3 (which itself is an extension of YAGO) that contains entities associated with at least ten different relations. In total, YAGO3-10 has 123,182 entities and 37 relations, and most of the triples describe attributes of persons such as citizenship, gender, and profession.\n",
    "\n",
    "https://paperswithcode.com/dataset/yago3-10\n",
    "\n",
    "* DBpedia: It extracts factual information from Wikipedia pages, allowing users to find answers to questions where the information is spread across multiple Wikipedia articles. Data is accessed using an SQL-like query language for RDF called SPARQL.\n",
    "\n",
    "https://www.dbpedia.org/\n",
    "\n",
    "* Wikidata : \n",
    "\n",
    "https://developer.ibm.com/articles/use-wikidata-in-ai-and-cognitive-applications-pt1/\n",
    "\n",
    "https://developer.ibm.com/articles/use-wikidata-in-ai-and-cognitive-applications-pt2/\n",
    "\n",
    "For this exercise we have remapped the IDs of freebase 237 and created a csv file containing human readable names instead of IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14060cfe-cbd4-4417-8a4d-b917b8472a42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>predicate</th>\n",
       "      <th>object</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>queens college, city university of new york</td>\n",
       "      <td>/education/educational_institution/students_gr...</td>\n",
       "      <td>carol leifer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>digital equipment corporation</td>\n",
       "      <td>/business/business_operation/industry</td>\n",
       "      <td>computer hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/m/0drtv8</td>\n",
       "      <td>/award/award_ceremony/awards_presented./award/...</td>\n",
       "      <td>laurence mark</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       subject  \\\n",
       "0  queens college, city university of new york   \n",
       "1                digital equipment corporation   \n",
       "2                                    /m/0drtv8   \n",
       "\n",
       "                                           predicate             object  \n",
       "0  /education/educational_institution/students_gr...       carol leifer  \n",
       "1              /business/business_operation/industry  computer hardware  \n",
       "2  /award/award_ceremony/awards_presented./award/...      laurence mark  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "URL = 'https://ampgraphenc.s3-eu-west-1.amazonaws.com/datasets/freebase-237-merged-and-remapped.csv'\n",
    "dataset = pd.read_csv(URL, header = None)\n",
    "dataset.columns = ['subject', 'predicate', 'object']\n",
    "dataset.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecca209e-a6c7-4e6b-803a-854350a5956e",
   "metadata": {},
   "source": [
    "One example -\n",
    "\n",
    "['academy award for best writing adapted screenplay',\n",
    "        '/award/award_category/nominees./award/award_nomination/nominated_for',\n",
    "        'the graduate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7cff934-8ee6-47b3-a1d0-9ef297f9dea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total triples in the KG:  (310079, 3)\n"
     ]
    }
   ],
   "source": [
    "print('Total triples in the KG: ', dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e9a831-9210-4d9f-9e9b-72128c15f787",
   "metadata": {},
   "source": [
    "# Creating training, validation and test splits\n",
    "\n",
    "We will use train_test_split_no_unseen(). This API ensures that the test and validation splits contain triples whose entities are \"seen\" during training . This API can be used to generate train/test splits such that test set contains only entities 'seen' during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c817a78-4fe5-4aa3-bdb5-b4cfa71be949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total triples:  (310079, 3)\n",
      "Size of train:  (308579, 3)\n",
      "Size of valid:  (500, 3)\n",
      "Size of test:  (1000, 3)\n"
     ]
    }
   ],
   "source": [
    "# Validation set of size 500\n",
    "\n",
    "test_train, X_valid = train_test_split_no_unseen(dataset.values, 500, seed = 0)\n",
    "\n",
    "# Test set of size 1000 from the remaining triples\n",
    "X_train, X_test = train_test_split_no_unseen(test_train, 1000, seed = 0)\n",
    "\n",
    "print('Total triples: ', dataset.shape)\n",
    "print('Size of train: ', X_train.shape)\n",
    "print('Size of valid: ', X_valid.shape)\n",
    "print('Size of test: ', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8858b0a-1230-44b3-b340-7d6f2943c4e0",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "Knowledge Graph embeddings are learned by training a neural architecture over a graph. In the training phase there is a loss function **L** that includes a scoring function **fm(t)** which is a model specific function that assigns a score to a triple **t = (sub, pred, obj)**\n",
    "\n",
    "https://docs.ampligraph.org/en/latest/ampligraph.latent_features.html\n",
    "\n",
    "a) **TransE** :  \n",
    "It uses simple vector algebra to score the triples. It has very low number of trainable parameters compared to most models. The scoring function computes a similarity between the embedding of the subject translated by the embedding of the predicate and embedding of the object using L1 or L2 norm ||.||\n",
    "\n",
    "                            f = −||s + p − o||n\n",
    "                            \n",
    "Translation Embeddings for modeling multi-relational data : https://proceedings.neurips.cc/paper/2013/file/1cecc7a77928ca8133fa24680a88d2f9-Paper.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "734f829a-7cce-4a6a-9fab-8693ae026ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransE(k = 150,                                                          # embedding size\n",
    "              epochs = 100,                                                      # num epochs\n",
    "              batches_count = 10,                                                # num batches\n",
    "              eta = 1,                                                           # num of corruptions to generate during training\n",
    "              loss = 'pairwise', loss_params = {'margin': 1},                    #  loss type and it's hyperparameters\n",
    "              initializer = 'xavier', initializer_params = {'uniform': False},\n",
    "              regularizer = 'LP', regularizer_params = {'lambda': 0.001, 'p': 3},\n",
    "              optimizer = 'adam', optimizer_params = {'lr': 0.001},\n",
    "              seed = 0, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cd528145-9e43-46d1-83d6-17dbe8d81cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average TransE Loss:   0.013576: 100%|████████████████████████████████████████████| 100/100 [02:31<00:00,  1.52s/epoch]\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train)\n",
    "\n",
    "save_model(model, 'TransE-small.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada04daa-e5d4-454c-8e6b-9afd972b33a1",
   "metadata": {},
   "source": [
    "# Compute the evaluation metrics-\n",
    "\n",
    "Per Triple metrics : This is a metric that is computed for each test triple."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06943101-80b2-435f-84e1-c62a3bf744d8",
   "metadata": {},
   "source": [
    "### Score : \n",
    "This is the value asigned to a triple, by the model, by applying the scoring function\n",
    "\n",
    "In order to interpret the score we have two options -\n",
    "\n",
    "1. We can create a list of hypothesis that we want to test, score them and then choose the top n hypothesis as True statements.\n",
    "2. Here unlike classification task, we are doing a learning to rank task. So, here we can generate the corruptions and compare the triple score against the scores of corruptions to see how well does the model rank the test triple against them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "36c03e64-0c85-4282-88bb-69f716e033d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triple of interest: \n",
      " ['harrison ford', '/film/actor/film./film/performance/film', 'star wars']\n",
      "Triple Score: \n",
      " [-8.3477955]\n"
     ]
    }
   ],
   "source": [
    "test_triple = ['harrison ford',\n",
    "              '/film/actor/film./film/performance/film',\n",
    "              'star wars']\n",
    "\n",
    "triple_score = model.predict(test_triple)\n",
    "\n",
    "print('Triple of interest: \\n', test_triple)\n",
    "print('Triple Score: \\n', triple_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fba33430-142c-4829-911a-c74bc050dc07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['salma hayek', '/film/actor/film./film/performance/film',\n",
       "        'star wars'],\n",
       "       ['carrie fisher', '/film/actor/film./film/performance/film',\n",
       "        'star wars'],\n",
       "       ['natalie portman', '/film/actor/film./film/performance/film',\n",
       "        'star wars'],\n",
       "       ['kristen bell', '/film/actor/film./film/performance/film',\n",
       "        'star wars'],\n",
       "       ['mark hamill', '/film/actor/film./film/performance/film',\n",
       "        'star wars'],\n",
       "       ['neil patrick harris', '/film/actor/film./film/performance/film',\n",
       "        'star wars'],\n",
       "       ['harrison ford', '/film/actor/film./film/performance/film',\n",
       "        'star wars']], dtype='<U39')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_actors = ['salma hayek', 'carrie fisher', 'natalie portman', 'kristen bell', 'mark hamill', 'neil patrick harris', 'harrison ford', ]\n",
    "\n",
    "# Stack it horizontally to create s, p, o\n",
    "hypothesis = np.column_stack([list_of_actors,\n",
    "                             ['/film/actor/film./film/performance/film'] * len(list_of_actors),\n",
    "                             ['star wars'] * len(list_of_actors)])\n",
    "hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2100d459-e85e-44fb-b2b3-c4e61e049cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "triple_scores = model.predict(hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3a315dc2-6de2-410d-895e-13953b84f58c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['natalie portman', '/film/actor/film./film/performance/film',\n",
       "        'star wars', '-8.2549305'],\n",
       "       ['harrison ford', '/film/actor/film./film/performance/film',\n",
       "        'star wars', '-8.3477955'],\n",
       "       ['carrie fisher', '/film/actor/film./film/performance/film',\n",
       "        'star wars', '-9.087776'],\n",
       "       ['neil patrick harris', '/film/actor/film./film/performance/film',\n",
       "        'star wars', '-9.221671'],\n",
       "       ['mark hamill', '/film/actor/film./film/performance/film',\n",
       "        'star wars', '-9.350438'],\n",
       "       ['kristen bell', '/film/actor/film./film/performance/film',\n",
       "        'star wars', '-9.354377'],\n",
       "       ['salma hayek', '/film/actor/film./film/performance/film',\n",
       "        'star wars', '-9.367626']], dtype='<U39')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# append the scores column\n",
    "scored_hypothesis = np.column_stack([hypothesis, triple_scores])\n",
    "# sort by score in descending order\n",
    "scored_hypothesis = scored_hypothesis[np.argsort(scored_hypothesis[:, 3])]\n",
    "scored_hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758041cc-d560-4025-b4b7-f060b7a459c0",
   "metadata": {},
   "source": [
    "### Rank\n",
    "For a triple, this metric is computed by generating corruptions and then scoring them and computing the rank(position) of the triple score against the corruptions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06a80e2-a84e-4ac7-94d4-2c183b904036",
   "metadata": {},
   "source": [
    "##### Step - 1 : Compute the score of the test triple\n",
    "The entire process should run in a loop which is -\n",
    "\n",
    "for each test set triple <s, p, o>:\n",
    "\n",
    "    # TODO\n",
    "    \n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a74d8d35-7fe1-47d4-8032-f40c2db4a1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triple of interest: \n",
      " ['harrison ford', '/film/actor/film./film/performance/film', 'star wars']\n",
      "Triple Score: \n",
      " [-8.3477955]\n"
     ]
    }
   ],
   "source": [
    "test_triple = ['harrison ford', \n",
    "               '/film/actor/film./film/performance/film', \n",
    "               'star wars']\n",
    "\n",
    "triple_score = model.predict(test_triple)\n",
    "\n",
    "print('Triple of interest: \\n', test_triple)\n",
    "print('Triple Score: \\n', triple_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57936a7c-397b-4aae-9780-0d7706a7116f",
   "metadata": {},
   "source": [
    "##### Step - 2 : Generate the Subject Corruptions and compute rank\n",
    "sub_corr = <?, p, o>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c175add2-dcce-4f3d-9bc1-e6dc01388353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique entities:  14184\n"
     ]
    }
   ],
   "source": [
    "print('The number of unique entities: ', len(model.ent_to_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fd3ff67e-c869-4132-8773-9b36bc5268ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_corr = np.column_stack([list(model.ent_to_idx.keys()),\n",
    "                            [test_triple[1]] * len(model.ent_to_idx),\n",
    "                            [test_triple[2]] * len(model.ent_to_idx)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6fc411bb-d248-4924-9653-9e19fcb3e5d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['/m/011xg5', '/film/actor/film./film/performance/film',\n",
       "        'star wars'],\n",
       "       ['/m/011yd2', '/film/actor/film./film/performance/film',\n",
       "        'star wars'],\n",
       "       ['/m/011yxg', '/film/actor/film./film/performance/film',\n",
       "        'star wars'],\n",
       "       ...,\n",
       "       ['zoology', '/film/actor/film./film/performance/film',\n",
       "        'star wars'],\n",
       "       ['zurich', '/film/actor/film./film/performance/film', 'star wars'],\n",
       "       ['zz top', '/film/actor/film./film/performance/film', 'star wars']],\n",
       "      dtype='<U107')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subj_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f9c09d5b-5d59-4ef9-8e89-4941c63d4aa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14184"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(subj_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5f4c9a60-c8d9-4df5-be89-349f4a3f136f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_corr_score = model.predict(subj_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b438ae5-f14e-4d21-a299-13476f3897ab",
   "metadata": {},
   "source": [
    "Now that we have a score, let us compute the rank as follows -\n",
    "\n",
    "COUNT(Corruption_score >= Triple_score)\n",
    "\n",
    "Find the position of the hypothesis score (triple score) in sub_corr_score to get the SUB_RANK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8c9df4e5-d31b-4e83-b0de-94fd6723115c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigning the worst rank (to break ties): 1529\n"
     ]
    }
   ],
   "source": [
    "sub_rank_worst = np.sum(np.greater_equal(sub_corr_score, triple_score[0])) + 1\n",
    "print('Assigning the worst rank (to break ties):', sub_rank_worst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad2bedc-d503-41bb-a390-b3d59e9e95b1",
   "metadata": {},
   "source": [
    "##### Step - 3 : Generate the Object Corruptions and compute rank\n",
    "\n",
    "obj_corr = <s, p, ?>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2602cf9a-e293-4e80-a1f4-46162cc60f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object corruptions:\n",
      " [['harrison ford' '/film/actor/film./film/performance/film' '/m/011xg5']\n",
      " ['harrison ford' '/film/actor/film./film/performance/film' '/m/011yd2']\n",
      " ['harrison ford' '/film/actor/film./film/performance/film' '/m/011yxg']\n",
      " ...\n",
      " ['harrison ford' '/film/actor/film./film/performance/film' 'zoology']\n",
      " ['harrison ford' '/film/actor/film./film/performance/film' 'zurich']\n",
      " ['harrison ford' '/film/actor/film./film/performance/film' 'zz top']]\n",
      "\n",
      "Size of object corruptions:\n",
      " (14184, 3)\n"
     ]
    }
   ],
   "source": [
    "obj_corr =  np.column_stack([\n",
    "                [test_triple[0]] * len(model.ent_to_idx),\n",
    "                [test_triple[1]] * len(model.ent_to_idx), \n",
    "                list(model.ent_to_idx.keys())])\n",
    "\n",
    "print('Object corruptions:\\n', obj_corr)\n",
    "print('\\nSize of object corruptions:\\n', obj_corr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3275a68d-2bf2-4fa5-958f-de6970f56676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the score of the object corruptions\n",
    "obj_corr_score = model.predict(obj_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "809f630f-ee4c-4d3c-96e3-39ed34b39785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigning the worst rank (to break ties): 832\n"
     ]
    }
   ],
   "source": [
    "# Find the position of hypothesis_score in obj_corr_score to get the obj_rank\n",
    "obj_rank_worst = np.sum(np.greater_equal(obj_corr_score, triple_score[0])) + 1\n",
    "print('Assigning the worst rank (to break ties):', obj_rank_worst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bd80a1de-af57-481f-a0f1-7c4a9725c0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject corruption rank: 1529\n",
      "Object corruption rank: 832\n"
     ]
    }
   ],
   "source": [
    "print('Subject corruption rank:', sub_rank_worst)\n",
    "print('Object corruption rank:', obj_rank_worst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cc507c-2cbe-4906-81c5-dd4b1827eb46",
   "metadata": {},
   "source": [
    "### Computing the (Unfiltered) rank using evaluate_performance API\n",
    "\n",
    "By default, evaluate_performance API computes the unfiltered ranls i.e. if any True positives are present in corruptions, they will not be removed before ranking. However, usually for evaluation we follow a filtered evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "010abaa7-21ac-41d7-bc81-52a94614a79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Ranks:  [[1529  832]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ranks = evaluate_performance(np.array([test_triple]),\n",
    "                            model = model,\n",
    "                            ranking_strategy = 'worst')\n",
    "\n",
    "print('\\n Ranks: ', ranks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f719b2-bc92-4fe1-ac67-dccb1c2c7897",
   "metadata": {},
   "source": [
    "###### Multiple strategies to compute rank when there are ties\n",
    "\n",
    "Assume there are only 10 corruptions, and assume that all corruptions get the same score as the test triple. The ranks are as follows -\n",
    "\n",
    "* Assign the **worst rank**, i.e. the test set triple gets a rank of 11. This is the strictest. This is used in ampligraph by default\n",
    "\n",
    "rank=COUNT(corruptionscore≥hypothesisscore)  + 1\n",
    "\n",
    "* **Middle Rank**,   the test set triple gets a rank of 6. Refer paper (ICLR 2020) : https://openreview.net/pdf?id=BkxSmlBFvr\n",
    "\n",
    "rank=COUNT(corruptionscore>hypothesisscore)+COUNT(corruptionscore==hypothesisscore)/2  + 1\n",
    "\n",
    "* **Best Rank**,  the test set triple gets a rank of 1. This approach is followed by ConvKB paper https://arxiv.org/pdf/1712.02121.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917c8da0-8298-4b80-b81c-12d8d97f3249",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ampligraph_env",
   "language": "python",
   "name": "ampligraph_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
