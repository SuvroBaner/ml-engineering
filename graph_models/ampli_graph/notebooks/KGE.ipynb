{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66976cae-76d7-40a3-8bdd-98df58733eca",
   "metadata": {},
   "source": [
    "## Importing Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfb68db9-aa9f-46ea-a457-663d8f7f3b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import ampligraph\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de82c15b-433b-4a35-8990-174008f518d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ampligraph.datasets import load_fb15k_237, load_wn18rr, load_yago3_10\n",
    "from ampligraph.evaluation import train_test_split_no_unseen, evaluate_performance, mr_score, mrr_score, hits_at_n_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3801eb2a-8b76-4c84-a494-3e8ed76dd654",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ampligraph.discovery import query_topn, discover_facts, find_clusters\n",
    "from ampligraph.latent_features import TransE, ComplEx, HolE, DistMult, ConvE, ConvKB\n",
    "from ampligraph.utils import save_model, restore_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "82455256-ab97-4b93-9323-1cb7607907af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ampligraph.evaluation import train_test_split_no_unseen\n",
    "from ampligraph.utils import save_model, restore_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26424381-3dac-46db-8a96-4376110c01d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ampligraph version : 1.4.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Ampligraph version : {}\".format(ampligraph.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d35ec47a-576b-4e96-a4ab-3216d2e93404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 1.15.5\n"
     ]
    }
   ],
   "source": [
    "print(\"Tensorflow version: {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cdad60-c079-479c-b0ee-e3563c0f0496",
   "metadata": {},
   "source": [
    "# Loading the KG (Knowledge Graph) dataset -\n",
    "\n",
    "A standard KG called **Freebase-15k-237** will be loaded. You can load KGs, csvs, ntriples etc from the API : https://docs.ampligraph.org/en/1.4.0/ampligraph.datasets.html\n",
    "\n",
    "* FB15k-237 dataset : Freebase knowledge base (ontology behind Google's semantic search feature (knowledge graph) which is a backend for Google search results that include structured asnwers to querues instead of series of links to external resources.) Its is 1.9 billion triples in the format (rfd - resource description format). Google bought it in 2010.IN 2016 it was closed and was migrated to Wikidata. FB15k-237 is a link prediction dataset created from FB15k. While FB15k consists of 1,345 relations, 14,951 entities, and 592,213 triples, many triples are inverses that cause leakage from the training to testing and validation splits. FB15k-237 was created by Toutanova and Chen (2015) to ensure that the testing and evaluation datasets do not have inverse relation test leakage. In summary, FB15k-237 dataset contains 310,079 triples with 14,505 entities and 237 relation types.\n",
    "\n",
    "https://paperswithcode.com/dataset/fb15k-237\n",
    "\n",
    "* wn18rr dataset : WN18RR is a link prediction dataset created from WN18, which is a subset of WordNet. WN18 consists of 18 relations and 40,943 entities. However, many text triples are obtained by inverting triples from the training set. Thus the WN18RR dataset is created to ensure that the evaluation dataset does not have inverse relation test leakage. In summary, WN18RR dataset contains 93,003 triples with 40,943 entities and 11 relation types.\n",
    "\n",
    "https://paperswithcode.com/dataset/wn18rr\n",
    "\n",
    "* yago3 : YAGO3-10 is benchmark dataset for knowledge base completion. It is a subset of YAGO3 (which itself is an extension of YAGO) that contains entities associated with at least ten different relations. In total, YAGO3-10 has 123,182 entities and 37 relations, and most of the triples describe attributes of persons such as citizenship, gender, and profession.\n",
    "\n",
    "https://paperswithcode.com/dataset/yago3-10\n",
    "\n",
    "* DBpedia: It extracts factual information from Wikipedia pages, allowing users to find answers to questions where the information is spread across multiple Wikipedia articles. Data is accessed using an SQL-like query language for RDF called SPARQL.\n",
    "\n",
    "https://www.dbpedia.org/\n",
    "\n",
    "* Wikidata : \n",
    "\n",
    "https://developer.ibm.com/articles/use-wikidata-in-ai-and-cognitive-applications-pt1/\n",
    "\n",
    "https://developer.ibm.com/articles/use-wikidata-in-ai-and-cognitive-applications-pt2/\n",
    "\n",
    "For this exercise we have remapped the IDs of freebase 237 and created a csv file containing human readable names instead of IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14060cfe-cbd4-4417-8a4d-b917b8472a42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>predicate</th>\n",
       "      <th>object</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>queens college, city university of new york</td>\n",
       "      <td>/education/educational_institution/students_gr...</td>\n",
       "      <td>carol leifer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>digital equipment corporation</td>\n",
       "      <td>/business/business_operation/industry</td>\n",
       "      <td>computer hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/m/0drtv8</td>\n",
       "      <td>/award/award_ceremony/awards_presented./award/...</td>\n",
       "      <td>laurence mark</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       subject  \\\n",
       "0  queens college, city university of new york   \n",
       "1                digital equipment corporation   \n",
       "2                                    /m/0drtv8   \n",
       "\n",
       "                                           predicate             object  \n",
       "0  /education/educational_institution/students_gr...       carol leifer  \n",
       "1              /business/business_operation/industry  computer hardware  \n",
       "2  /award/award_ceremony/awards_presented./award/...      laurence mark  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "URL = 'https://ampgraphenc.s3-eu-west-1.amazonaws.com/datasets/freebase-237-merged-and-remapped.csv'\n",
    "dataset = pd.read_csv(URL, header = None)\n",
    "dataset.columns = ['subject', 'predicate', 'object']\n",
    "dataset.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecca209e-a6c7-4e6b-803a-854350a5956e",
   "metadata": {},
   "source": [
    "One example -\n",
    "\n",
    "['academy award for best writing adapted screenplay',\n",
    "        '/award/award_category/nominees./award/award_nomination/nominated_for',\n",
    "        'the graduate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7cff934-8ee6-47b3-a1d0-9ef297f9dea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total triples in the KG:  (310079, 3)\n"
     ]
    }
   ],
   "source": [
    "print('Total triples in the KG: ', dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e9a831-9210-4d9f-9e9b-72128c15f787",
   "metadata": {},
   "source": [
    "# Creating training, validation and test splits\n",
    "\n",
    "We will use train_test_split_no_unseen(). This API ensures that the test and validation splits contain triples whose entities are \"seen\" during training . This API can be used to generate train/test splits such that test set contains only entities 'seen' during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c817a78-4fe5-4aa3-bdb5-b4cfa71be949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total triples:  (310079, 3)\n",
      "Size of train:  (308579, 3)\n",
      "Size of valid:  (500, 3)\n",
      "Size of test:  (1000, 3)\n"
     ]
    }
   ],
   "source": [
    "# Validation set of size 500\n",
    "\n",
    "test_train, X_valid = train_test_split_no_unseen(dataset.values, 500, seed = 0)\n",
    "\n",
    "# Test set of size 1000 from the remaining triples\n",
    "X_train, X_test = train_test_split_no_unseen(test_train, 1000, seed = 0)\n",
    "\n",
    "print('Total triples: ', dataset.shape)\n",
    "print('Size of train: ', X_train.shape)\n",
    "print('Size of valid: ', X_valid.shape)\n",
    "print('Size of test: ', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8858b0a-1230-44b3-b340-7d6f2943c4e0",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "Knowledge Graph embeddings are learned by training a neural architecture over a graph. In the training phase there is a loss function **L** that includes a scoring function **fm(t)** which is a model specific function that assigns a score to a triple **t = (sub, pred, obj)**\n",
    "\n",
    "https://docs.ampligraph.org/en/latest/ampligraph.latent_features.html\n",
    "\n",
    "a) **TransE** :  \n",
    "It uses simple vector algebra to score the triples. It has very low number of trainable parameters compared to most models. The scoring function computes a similarity between the embedding of the subject translated by the embedding of the predicate and embedding of the object using L1 or L2 norm ||.||\n",
    "\n",
    "                            f = −||s + p − o||n\n",
    "                            \n",
    "Translation Embeddings for modeling multi-relational data : https://proceedings.neurips.cc/paper/2013/file/1cecc7a77928ca8133fa24680a88d2f9-Paper.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "734f829a-7cce-4a6a-9fab-8693ae026ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransE(k = 150,                                                          # embedding size\n",
    "              epochs = 100,                                                      # num epochs\n",
    "              batches_count = 10,                                                # num batches\n",
    "              eta = 1,                                                           # num of corruptions to generate during training\n",
    "              loss = 'pairwise', loss_params = {'margin': 1},                    #  loss type and it's hyperparameters\n",
    "              initializer = 'xavier', initializer_params = {'uniform': False},\n",
    "              regularizer = 'LP', regularizer_params = {'lambda': 0.001, 'p': 3},\n",
    "              optimizer = 'adam', optimizer_params = {'lr': 0.001},\n",
    "              seed = 0, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cd528145-9e43-46d1-83d6-17dbe8d81cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average TransE Loss:   0.013576: 100%|████████████████████████████████████████████| 100/100 [02:31<00:00,  1.52s/epoch]\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train)\n",
    "\n",
    "save_model(model, 'TransE-small.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226968f4-f9ab-4963-866e-6cae8890e205",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ampligraph_env",
   "language": "python",
   "name": "ampligraph_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
