{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66976cae-76d7-40a3-8bdd-98df58733eca",
   "metadata": {},
   "source": [
    "## Importing Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfb68db9-aa9f-46ea-a457-663d8f7f3b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import ampligraph\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de82c15b-433b-4a35-8990-174008f518d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ampligraph.datasets import load_fb15k_237, load_wn18rr, load_yago3_10\n",
    "from ampligraph.evaluation import train_test_split_no_unseen, evaluate_performance, mr_score, mrr_score, hits_at_n_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3801eb2a-8b76-4c84-a494-3e8ed76dd654",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ampligraph.discovery import query_topn, discover_facts, find_clusters\n",
    "from ampligraph.latent_features import TransE, ComplEx, HolE, DistMult, ConvE, ConvKB\n",
    "from ampligraph.utils import save_model, restore_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "82455256-ab97-4b93-9323-1cb7607907af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ampligraph.evaluation import train_test_split_no_unseen\n",
    "from ampligraph.utils import save_model, restore_model\n",
    "from ampligraph.evaluation import evaluate_performance\n",
    "from ampligraph.evaluation import mr_score\n",
    "from ampligraph.evaluation import mrr_score\n",
    "from ampligraph.evaluation import hits_at_n_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26424381-3dac-46db-8a96-4376110c01d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ampligraph version : 1.4.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Ampligraph version : {}\".format(ampligraph.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d35ec47a-576b-4e96-a4ab-3216d2e93404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 1.15.5\n"
     ]
    }
   ],
   "source": [
    "print(\"Tensorflow version: {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cdad60-c079-479c-b0ee-e3563c0f0496",
   "metadata": {},
   "source": [
    "# Loading the KG (Knowledge Graph) dataset -\n",
    "\n",
    "A standard KG called **Freebase-15k-237** will be loaded. You can load KGs, csvs, ntriples etc from the API : https://docs.ampligraph.org/en/1.4.0/ampligraph.datasets.html\n",
    "\n",
    "* FB15k-237 dataset : Freebase knowledge base (ontology behind Google's semantic search feature (knowledge graph) which is a backend for Google search results that include structured asnwers to querues instead of series of links to external resources.) Its is 1.9 billion triples in the format (rfd - resource description format). Google bought it in 2010.IN 2016 it was closed and was migrated to Wikidata. FB15k-237 is a link prediction dataset created from FB15k. While FB15k consists of 1,345 relations, 14,951 entities, and 592,213 triples, many triples are inverses that cause leakage from the training to testing and validation splits. FB15k-237 was created by Toutanova and Chen (2015) to ensure that the testing and evaluation datasets do not have inverse relation test leakage. In summary, FB15k-237 dataset contains 310,079 triples with 14,505 entities and 237 relation types.\n",
    "\n",
    "https://paperswithcode.com/dataset/fb15k-237\n",
    "\n",
    "* wn18rr dataset : WN18RR is a link prediction dataset created from WN18, which is a subset of WordNet. WN18 consists of 18 relations and 40,943 entities. However, many text triples are obtained by inverting triples from the training set. Thus the WN18RR dataset is created to ensure that the evaluation dataset does not have inverse relation test leakage. In summary, WN18RR dataset contains 93,003 triples with 40,943 entities and 11 relation types.\n",
    "\n",
    "https://paperswithcode.com/dataset/wn18rr\n",
    "\n",
    "* yago3 : YAGO3-10 is benchmark dataset for knowledge base completion. It is a subset of YAGO3 (which itself is an extension of YAGO) that contains entities associated with at least ten different relations. In total, YAGO3-10 has 123,182 entities and 37 relations, and most of the triples describe attributes of persons such as citizenship, gender, and profession.\n",
    "\n",
    "https://paperswithcode.com/dataset/yago3-10\n",
    "\n",
    "* DBpedia: It extracts factual information from Wikipedia pages, allowing users to find answers to questions where the information is spread across multiple Wikipedia articles. Data is accessed using an SQL-like query language for RDF called SPARQL.\n",
    "\n",
    "https://www.dbpedia.org/\n",
    "\n",
    "* Wikidata : \n",
    "\n",
    "https://developer.ibm.com/articles/use-wikidata-in-ai-and-cognitive-applications-pt1/\n",
    "\n",
    "https://developer.ibm.com/articles/use-wikidata-in-ai-and-cognitive-applications-pt2/\n",
    "\n",
    "For this exercise we have remapped the IDs of freebase 237 and created a csv file containing human readable names instead of IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14060cfe-cbd4-4417-8a4d-b917b8472a42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>predicate</th>\n",
       "      <th>object</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>queens college, city university of new york</td>\n",
       "      <td>/education/educational_institution/students_gr...</td>\n",
       "      <td>carol leifer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>digital equipment corporation</td>\n",
       "      <td>/business/business_operation/industry</td>\n",
       "      <td>computer hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/m/0drtv8</td>\n",
       "      <td>/award/award_ceremony/awards_presented./award/...</td>\n",
       "      <td>laurence mark</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       subject  \\\n",
       "0  queens college, city university of new york   \n",
       "1                digital equipment corporation   \n",
       "2                                    /m/0drtv8   \n",
       "\n",
       "                                           predicate             object  \n",
       "0  /education/educational_institution/students_gr...       carol leifer  \n",
       "1              /business/business_operation/industry  computer hardware  \n",
       "2  /award/award_ceremony/awards_presented./award/...      laurence mark  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "URL = 'https://ampgraphenc.s3-eu-west-1.amazonaws.com/datasets/freebase-237-merged-and-remapped.csv'\n",
    "dataset = pd.read_csv(URL, header = None)\n",
    "dataset.columns = ['subject', 'predicate', 'object']\n",
    "dataset.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecca209e-a6c7-4e6b-803a-854350a5956e",
   "metadata": {},
   "source": [
    "One example -\n",
    "\n",
    "['academy award for best writing adapted screenplay',\n",
    "        '/award/award_category/nominees./award/award_nomination/nominated_for',\n",
    "        'the graduate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7cff934-8ee6-47b3-a1d0-9ef297f9dea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total triples in the KG:  (310079, 3)\n"
     ]
    }
   ],
   "source": [
    "print('Total triples in the KG: ', dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e9a831-9210-4d9f-9e9b-72128c15f787",
   "metadata": {},
   "source": [
    "# Creating training, validation and test splits\n",
    "\n",
    "We will use train_test_split_no_unseen(). This API ensures that the test and validation splits contain triples whose entities are \"seen\" during training . This API can be used to generate train/test splits such that test set contains only entities 'seen' during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c817a78-4fe5-4aa3-bdb5-b4cfa71be949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total triples:  (310079, 3)\n",
      "Size of train:  (308579, 3)\n",
      "Size of valid:  (500, 3)\n",
      "Size of test:  (1000, 3)\n"
     ]
    }
   ],
   "source": [
    "# Validation set of size 500\n",
    "\n",
    "test_train, X_valid = train_test_split_no_unseen(dataset.values, 500, seed = 0)\n",
    "\n",
    "# Test set of size 1000 from the remaining triples\n",
    "X_train, X_test = train_test_split_no_unseen(test_train, 1000, seed = 0)\n",
    "\n",
    "print('Total triples: ', dataset.shape)\n",
    "print('Size of train: ', X_train.shape)\n",
    "print('Size of valid: ', X_valid.shape)\n",
    "print('Size of test: ', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8858b0a-1230-44b3-b340-7d6f2943c4e0",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "Knowledge Graph embeddings are learned by training a neural architecture over a graph. In the training phase there is a loss function **L** that includes a scoring function **fm(t)** which is a model specific function that assigns a score to a triple **t = (sub, pred, obj)**\n",
    "\n",
    "https://docs.ampligraph.org/en/latest/ampligraph.latent_features.html\n",
    "\n",
    "a) **TransE** :  \n",
    "It uses simple vector algebra to score the triples. It has very low number of trainable parameters compared to most models. The scoring function computes a similarity between the embedding of the subject translated by the embedding of the predicate and embedding of the object using L1 or L2 norm ||.||\n",
    "\n",
    "                            f = −||s + p − o||n\n",
    "                            \n",
    "Translation Embeddings for modeling multi-relational data : https://proceedings.neurips.cc/paper/2013/file/1cecc7a77928ca8133fa24680a88d2f9-Paper.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "734f829a-7cce-4a6a-9fab-8693ae026ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransE(k = 150,                                                          # embedding size\n",
    "              epochs = 100,                                                      # num epochs\n",
    "              batches_count = 10,                                                # num batches\n",
    "              eta = 1,                                                           # num of corruptions to generate during training\n",
    "              loss = 'pairwise', loss_params = {'margin': 1},                    #  loss type and it's hyperparameters\n",
    "              initializer = 'xavier', initializer_params = {'uniform': False},\n",
    "              regularizer = 'LP', regularizer_params = {'lambda': 0.001, 'p': 3},\n",
    "              optimizer = 'adam', optimizer_params = {'lr': 0.001},\n",
    "              seed = 0, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cd528145-9e43-46d1-83d6-17dbe8d81cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average TransE Loss:   0.013576: 100%|████████████████████████████████████████████| 100/100 [02:31<00:00,  1.52s/epoch]\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train)\n",
    "\n",
    "save_model(model, 'TransE-small.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada04daa-e5d4-454c-8e6b-9afd972b33a1",
   "metadata": {},
   "source": [
    "# Compute the evaluation metrics-\n",
    "\n",
    "Per Triple metrics : This is a metric that is computed for each test triple."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06943101-80b2-435f-84e1-c62a3bf744d8",
   "metadata": {},
   "source": [
    "### Score : \n",
    "This is the value asigned to a triple, by the model, by applying the scoring function\n",
    "\n",
    "In order to interpret the score we have two options -\n",
    "\n",
    "1. We can create a list of hypothesis that we want to test, score them and then choose the top n hypothesis as True statements.\n",
    "2. Here unlike classification task, we are doing a learning to rank task. So, here we can generate the corruptions and compare the triple score against the scores of corruptions to see how well does the model rank the test triple against them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "36c03e64-0c85-4282-88bb-69f716e033d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triple of interest: \n",
      " ['harrison ford', '/film/actor/film./film/performance/film', 'star wars']\n",
      "Triple Score: \n",
      " [-8.3477955]\n"
     ]
    }
   ],
   "source": [
    "test_triple = ['harrison ford',\n",
    "              '/film/actor/film./film/performance/film',\n",
    "              'star wars']\n",
    "\n",
    "triple_score = model.predict(test_triple)\n",
    "\n",
    "print('Triple of interest: \\n', test_triple)\n",
    "print('Triple Score: \\n', triple_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fba33430-142c-4829-911a-c74bc050dc07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['salma hayek', '/film/actor/film./film/performance/film',\n",
       "        'star wars'],\n",
       "       ['carrie fisher', '/film/actor/film./film/performance/film',\n",
       "        'star wars'],\n",
       "       ['natalie portman', '/film/actor/film./film/performance/film',\n",
       "        'star wars'],\n",
       "       ['kristen bell', '/film/actor/film./film/performance/film',\n",
       "        'star wars'],\n",
       "       ['mark hamill', '/film/actor/film./film/performance/film',\n",
       "        'star wars'],\n",
       "       ['neil patrick harris', '/film/actor/film./film/performance/film',\n",
       "        'star wars'],\n",
       "       ['harrison ford', '/film/actor/film./film/performance/film',\n",
       "        'star wars']], dtype='<U39')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_actors = ['salma hayek', 'carrie fisher', 'natalie portman', 'kristen bell', 'mark hamill', 'neil patrick harris', 'harrison ford', ]\n",
    "\n",
    "# Stack it horizontally to create s, p, o\n",
    "hypothesis = np.column_stack([list_of_actors,\n",
    "                             ['/film/actor/film./film/performance/film'] * len(list_of_actors),\n",
    "                             ['star wars'] * len(list_of_actors)])\n",
    "hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2100d459-e85e-44fb-b2b3-c4e61e049cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "triple_scores = model.predict(hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3a315dc2-6de2-410d-895e-13953b84f58c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['natalie portman', '/film/actor/film./film/performance/film',\n",
       "        'star wars', '-8.2549305'],\n",
       "       ['harrison ford', '/film/actor/film./film/performance/film',\n",
       "        'star wars', '-8.3477955'],\n",
       "       ['carrie fisher', '/film/actor/film./film/performance/film',\n",
       "        'star wars', '-9.087776'],\n",
       "       ['neil patrick harris', '/film/actor/film./film/performance/film',\n",
       "        'star wars', '-9.221671'],\n",
       "       ['mark hamill', '/film/actor/film./film/performance/film',\n",
       "        'star wars', '-9.350438'],\n",
       "       ['kristen bell', '/film/actor/film./film/performance/film',\n",
       "        'star wars', '-9.354377'],\n",
       "       ['salma hayek', '/film/actor/film./film/performance/film',\n",
       "        'star wars', '-9.367626']], dtype='<U39')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# append the scores column\n",
    "scored_hypothesis = np.column_stack([hypothesis, triple_scores])\n",
    "# sort by score in descending order\n",
    "scored_hypothesis = scored_hypothesis[np.argsort(scored_hypothesis[:, 3])]\n",
    "scored_hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758041cc-d560-4025-b4b7-f060b7a459c0",
   "metadata": {},
   "source": [
    "### Rank\n",
    "For a triple, this metric is computed by generating corruptions and then scoring them and computing the rank(position) of the triple score against the corruptions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06a80e2-a84e-4ac7-94d4-2c183b904036",
   "metadata": {},
   "source": [
    "##### Step - 1 : Compute the score of the test triple\n",
    "The entire process should run in a loop which is -\n",
    "\n",
    "for each test set triple <s, p, o>:\n",
    "           \n",
    "           a. Compute the score of the test triple (hypothesis) \n",
    "               hypothesis_score = score(<s, p, o>)\n",
    "\n",
    "           b. Generate the subject corruptions \n",
    "                   sub_corr = <?, p, o>\n",
    "           c. Compute the score of the subject corruptions\n",
    "                   sub_corr_score = score(sub_corr) \n",
    "           d. Find the position of hypothesis_score in sub_corr_score to get the sub_rank\n",
    "\n",
    "           e. Generate the object corruption \n",
    "                   obj_corr = <s, p, ?>\n",
    "           f. Compute the score of the object corruptions\n",
    "                   obj_corr_score = score(obj_corr) \n",
    "           g. Find the position of hypothesis_score in obj_corr_score to get the obj_rank\n",
    "\n",
    "           h. Return rank = [sub_rank, obj_rank]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a74d8d35-7fe1-47d4-8032-f40c2db4a1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triple of interest: \n",
      " ['harrison ford', '/film/actor/film./film/performance/film', 'star wars']\n",
      "Triple Score: \n",
      " [-8.3477955]\n"
     ]
    }
   ],
   "source": [
    "test_triple = ['harrison ford', \n",
    "               '/film/actor/film./film/performance/film', \n",
    "               'star wars']\n",
    "\n",
    "triple_score = model.predict(test_triple)\n",
    "\n",
    "print('Triple of interest: \\n', test_triple)\n",
    "print('Triple Score: \\n', triple_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57936a7c-397b-4aae-9780-0d7706a7116f",
   "metadata": {},
   "source": [
    "##### Step - 2 : Generate the Subject Corruptions and compute rank\n",
    "sub_corr = <?, p, o>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c175add2-dcce-4f3d-9bc1-e6dc01388353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique entities:  14184\n"
     ]
    }
   ],
   "source": [
    "print('The number of unique entities: ', len(model.ent_to_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fd3ff67e-c869-4132-8773-9b36bc5268ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_corr = np.column_stack([list(model.ent_to_idx.keys()),\n",
    "                            [test_triple[1]] * len(model.ent_to_idx),\n",
    "                            [test_triple[2]] * len(model.ent_to_idx)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6fc411bb-d248-4924-9653-9e19fcb3e5d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['/m/011xg5', '/film/actor/film./film/performance/film',\n",
       "        'star wars'],\n",
       "       ['/m/011yd2', '/film/actor/film./film/performance/film',\n",
       "        'star wars'],\n",
       "       ['/m/011yxg', '/film/actor/film./film/performance/film',\n",
       "        'star wars'],\n",
       "       ...,\n",
       "       ['zoology', '/film/actor/film./film/performance/film',\n",
       "        'star wars'],\n",
       "       ['zurich', '/film/actor/film./film/performance/film', 'star wars'],\n",
       "       ['zz top', '/film/actor/film./film/performance/film', 'star wars']],\n",
       "      dtype='<U107')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subj_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f9c09d5b-5d59-4ef9-8e89-4941c63d4aa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14184"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(subj_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5f4c9a60-c8d9-4df5-be89-349f4a3f136f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_corr_score = model.predict(subj_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b438ae5-f14e-4d21-a299-13476f3897ab",
   "metadata": {},
   "source": [
    "Now that we have a score, let us compute the rank as follows -\n",
    "\n",
    "COUNT(Corruption_score >= Triple_score)\n",
    "\n",
    "Find the position of the hypothesis score (triple score) in sub_corr_score to get the SUB_RANK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8c9df4e5-d31b-4e83-b0de-94fd6723115c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigning the worst rank (to break ties): 1529\n"
     ]
    }
   ],
   "source": [
    "sub_rank_worst = np.sum(np.greater_equal(sub_corr_score, triple_score[0])) + 1\n",
    "print('Assigning the worst rank (to break ties):', sub_rank_worst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad2bedc-d503-41bb-a390-b3d59e9e95b1",
   "metadata": {},
   "source": [
    "##### Step - 3 : Generate the Object Corruptions and compute rank\n",
    "\n",
    "obj_corr = <s, p, ?>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2602cf9a-e293-4e80-a1f4-46162cc60f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object corruptions:\n",
      " [['harrison ford' '/film/actor/film./film/performance/film' '/m/011xg5']\n",
      " ['harrison ford' '/film/actor/film./film/performance/film' '/m/011yd2']\n",
      " ['harrison ford' '/film/actor/film./film/performance/film' '/m/011yxg']\n",
      " ...\n",
      " ['harrison ford' '/film/actor/film./film/performance/film' 'zoology']\n",
      " ['harrison ford' '/film/actor/film./film/performance/film' 'zurich']\n",
      " ['harrison ford' '/film/actor/film./film/performance/film' 'zz top']]\n",
      "\n",
      "Size of object corruptions:\n",
      " (14184, 3)\n"
     ]
    }
   ],
   "source": [
    "obj_corr =  np.column_stack([\n",
    "                [test_triple[0]] * len(model.ent_to_idx),\n",
    "                [test_triple[1]] * len(model.ent_to_idx), \n",
    "                list(model.ent_to_idx.keys())])\n",
    "\n",
    "print('Object corruptions:\\n', obj_corr)\n",
    "print('\\nSize of object corruptions:\\n', obj_corr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3275a68d-2bf2-4fa5-958f-de6970f56676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the score of the object corruptions\n",
    "obj_corr_score = model.predict(obj_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "809f630f-ee4c-4d3c-96e3-39ed34b39785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigning the worst rank (to break ties): 832\n"
     ]
    }
   ],
   "source": [
    "# Find the position of hypothesis_score in obj_corr_score to get the obj_rank\n",
    "obj_rank_worst = np.sum(np.greater_equal(obj_corr_score, triple_score[0])) + 1\n",
    "print('Assigning the worst rank (to break ties):', obj_rank_worst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bd80a1de-af57-481f-a0f1-7c4a9725c0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject corruption rank: 1529\n",
      "Object corruption rank: 832\n"
     ]
    }
   ],
   "source": [
    "print('Subject corruption rank:', sub_rank_worst)\n",
    "print('Object corruption rank:', obj_rank_worst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cc507c-2cbe-4906-81c5-dd4b1827eb46",
   "metadata": {},
   "source": [
    "### Computing the (Unfiltered) rank using evaluate_performance API\n",
    "\n",
    "By default, evaluate_performance API computes the unfiltered ranls i.e. if any True positives are present in corruptions, they will not be removed before ranking. However, usually for evaluation we follow a filtered evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "010abaa7-21ac-41d7-bc81-52a94614a79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Ranks:  [[1529  832]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ranks = evaluate_performance(np.array([test_triple]),\n",
    "                            model = model,\n",
    "                            ranking_strategy = 'worst')\n",
    "\n",
    "print('\\n Ranks: ', ranks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f719b2-bc92-4fe1-ac67-dccb1c2c7897",
   "metadata": {},
   "source": [
    "###### Multiple strategies to compute rank when there are ties\n",
    "\n",
    "Assume there are only 10 corruptions, and assume that all corruptions get the same score as the test triple. The ranks are as follows -\n",
    "\n",
    "* Assign the **worst rank**, i.e. the test set triple gets a rank of 11. This is the strictest. This is used in ampligraph by default\n",
    "\n",
    "rank=COUNT(corruptionscore≥hypothesisscore)  + 1\n",
    "\n",
    "* **Middle Rank**,   the test set triple gets a rank of 6. Refer paper (ICLR 2020) : https://openreview.net/pdf?id=BkxSmlBFvr\n",
    "\n",
    "rank=COUNT(corruptionscore>hypothesisscore)+COUNT(corruptionscore==hypothesisscore)/2  + 1\n",
    "\n",
    "* **Best Rank**,  the test set triple gets a rank of 1. This approach is followed by ConvKB paper https://arxiv.org/pdf/1712.02121.pdf\n",
    "\n",
    "rank=COUNT(corruptionscore>hypothesisscore)  + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f09dbd-3f06-4bfc-9049-789876739fb1",
   "metadata": {},
   "source": [
    "### Filtered Evaluation - \n",
    "\n",
    "Previously while evaluating, we generate all the corruptions (using all unique entities in our datasets) per test triple, score and rank them. While doing so we are not filtering the true posiives (i.e. some corruptions may not really be corruptions and may be ground truth triples observed during training.) Training triples usually get a high score as they are \"observed\" by the model. Hence a test triple would get a lower rank if such triples appear in corruptions. \n",
    "\n",
    "To filter out the True Positives (after generating subject corruptions and after generating object corruptions). one can pass all the True Positive triples to filter_triples parameter of the evaluate_performance API. This will perform a \"filtered\" evaluation and return the \"filtered\" ranks adjusted by removing the True Positives from the corruptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cb3a9e3c-f231-4fe8-ab10-1bf0664e28b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X_test:  (1000, 3)\n"
     ]
    }
   ],
   "source": [
    "print('Size of X_test: ', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "00eaa4d7-40b0-4cb8-81ef-cae7aecd2d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_filter = np.concatenate([X_train, X_valid, X_test], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bab4b60b-73d4-412d-ae7f-8f9635512fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1518  822]]\n"
     ]
    }
   ],
   "source": [
    "ranks = evaluate_performance(np.array([test_triple]),\n",
    "                            model = model,\n",
    "                            filter_triples = X_filter)\n",
    "print(ranks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e576d8-1a9f-4664-853b-a6adbf7fbb0e",
   "metadata": {},
   "source": [
    "One obvious question is why do we append the Valid and Test set to the X_filter. The model has not \"observed\" them during training. We do so because, we would like to evaluate a test triple against it's corruptions and not against known facts. If we know that the Validation triples and Test triples are facts (and not queries), we need to filter these triples out of the generated corruptions. This is the standard procedure that is used to compute the metrics to compete on the leadership board."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840b668b-123b-4767-a206-bb99203eeedd",
   "metadata": {},
   "source": [
    "### Aggregate Metrics\n",
    "\n",
    "Once we have the ranks for all the test set triples, we can compute the following aggregate metrics :\n",
    "* MR\n",
    "* MRR\n",
    "* Hits@N\n",
    "\n",
    "These metrics indicate the overall quality of the model on a test set. These metrics come from Information Retrieval domain and are always computed on a set of True Statements.\n",
    "Let's do it with an example -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5fc4f69e-45e1-416d-8ca7-7fc0ea702e68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['doctorate',\n",
       "        '/education/educational_degree/people_with_this_degree./education/education/major_field_of_study',\n",
       "        'computer engineering'],\n",
       "       ['star wars',\n",
       "        '/film/film/estimated_budget./measurement_unit/dated_money_value/currency',\n",
       "        'united states dollar'],\n",
       "       ['harry potter and the chamber of secrets',\n",
       "        '/film/film/estimated_budget./measurement_unit/dated_money_value/currency',\n",
       "        'united states dollar'],\n",
       "       ['star wars', '/film/film/language', 'english language'],\n",
       "       ['harrison ford', '/film/actor/film./film/performance/film',\n",
       "        'star wars']], dtype='<U95')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_small = np.array(\n",
    "                [['doctorate',\n",
    "                    '/education/educational_degree/people_with_this_degree./education/education/major_field_of_study',\n",
    "                    'computer engineering'],\n",
    "                 ['star wars',\n",
    "                    '/film/film/estimated_budget./measurement_unit/dated_money_value/currency',\n",
    "                    'united states dollar'],\n",
    "                 ['harry potter and the chamber of secrets',\n",
    "                    '/film/film/estimated_budget./measurement_unit/dated_money_value/currency',\n",
    "                    'united states dollar'],\n",
    "                 ['star wars', '/film/film/language', 'english language'],\n",
    "                 ['harrison ford', '/film/actor/film./film/performance/film', 'star wars']\n",
    "                ])\n",
    "X_test_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "26218182-e3b0-4e09-8ec7-bcabd41818b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "308579"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "699ac990-7e40-44f1-bab5-b02fd9e3c810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "752fccbb-687b-48fd-a923-c0dd9aa93597",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 18.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   8    9]\n",
      " [   1    1]\n",
      " [ 109    1]\n",
      " [   1    1]\n",
      " [1518  822]]\n"
     ]
    }
   ],
   "source": [
    "X_filter = np.concatenate([X_train, X_valid, X_test], 0)\n",
    "\n",
    "ranks = evaluate_performance(X_test_small,\n",
    "                            model = model,\n",
    "                            filter_triples = X_filter,\n",
    "                            corrupt_side = 's,o')\n",
    "print(ranks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378e111a-2047-4035-979a-534061fe5876",
   "metadata": {},
   "source": [
    "#### Mean Rank (MR) : \n",
    "It is the average of all the ranks of the triples. The value ranges from 1 (ideal case when all ranks equal to 1) to number of corruptions (where all the ranks are last)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "891735d2-3c02-43c5-ac67-68af54a3eac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MR : 247.1\n"
     ]
    }
   ],
   "source": [
    "print('MR :', mr_score(ranks))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff52687-2e14-4990-b667-cff366cc1ea2",
   "metadata": {},
   "source": [
    "#### Mean Reciprocal Rank (MRR) : \n",
    "It is the average of the reciprocal ranks of all the triples. The value ranges from 0 to 1; higher the value better is the model.\n",
    "MRR is an indicator of mean rank after removing the effect of outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c0e84942-d561-4dcc-9a8d-448434af73e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR : 0.5247160729578209\n"
     ]
    }
   ],
   "source": [
    "print('MRR :', mrr_score(ranks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2ee7b857-663c-47d2-b2e8-bfb5c51c753f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean rank after removing the outlier effect:  2.0\n"
     ]
    }
   ],
   "source": [
    "print('Mean rank after removing the outlier effect: ', np.ceil(1/mrr_score(ranks)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63d0318-55f1-4d33-ab9d-2d24d1cec833",
   "metadata": {},
   "source": [
    "#### Hits@n\n",
    "It is the percentage of computed ranks that are greater than (in terms of ranking) or equal to a rank of n. The value ranges from 0 to 1; higher the value better is the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4c020ab8-9266-4977-8b8a-4a899d076a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hits@1 : 0.5\n",
      "hits@10 : 0.7\n"
     ]
    }
   ],
   "source": [
    "print('hits@1 :', hits_at_n_score(ranks, 1))\n",
    "print('hits@10 :', hits_at_n_score(ranks, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "da567c31-2762-4a04-9288-fbb507a92727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique entities: 14184\n"
     ]
    }
   ],
   "source": [
    "# print unique entities\n",
    "print('Number of unique entities:', len(model.ent_to_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04dfecab-1971-4be0-818e-128e359b423c",
   "metadata": {},
   "source": [
    "What if, for a model, you observe that on a test set, the MRR score is 0.01? Is it a good model?\n",
    "\n",
    "It is not very straightforward. What the above value means is that if you remove the outlier effect, on an average the ranks are around 100 (1/0.01). It may be a good/bad value. It depends on number of corruptions that you have used for the computation. Say you had 1 million corruptions and yet the mrr score was 0.01. The model, in general, was quite good at ranking against 1 million corruption because on an average it gave a rank of close to 100. But say if the corruptions were only 100 and we had an mrr of 0.01, it means that the model did a very bad task at ranking the test triples against just 100 corruptions.\n",
    "\n",
    "On a real life dataset, on should take a closer look at hits@n values and decide whether the model is a good model or not. The choice of n should depend on the number of corruptions that are being generated per test triple. If a large percentage of ranks computed on the test set triple falls within the n ranks, then the model can be considered as a good model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7e1898f7-afe7-43a2-bd01-6feb9974cedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_aggregate_metrics(ranks):\n",
    "    print('Mean Rank:', mr_score(ranks)) \n",
    "    print('Mean Reciprocal Rank:', mrr_score(ranks)) \n",
    "    print('Hits@1:', hits_at_n_score(ranks, 1))\n",
    "    print('Hits@10:', hits_at_n_score(ranks, 10))\n",
    "    print('Hits@100:', hits_at_n_score(ranks, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d4e788ef-1e6d-40c6-8794-4257515d3aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Rank: 247.1\n",
      "Mean Reciprocal Rank: 0.5247160729578209\n",
      "Hits@1: 0.5\n",
      "Hits@10: 0.7\n",
      "Hits@100: 0.7\n"
     ]
    }
   ],
   "source": [
    "display_aggregate_metrics(ranks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5c7f0e-a763-4a0b-91ba-15365a9274c3",
   "metadata": {},
   "source": [
    "# Training with Early Stopping \n",
    "\n",
    "While training a model, we would like to make sure that the model does not overfit or under fit on the data. If we train a model for a fixed number of epochs, we will not know whether the model has underfit or overfit the training data. Hence it is necessary to test the model performance on a held out set at regular intervals to decide when to stop training. This is called \"Early stopping\", i.e. we don't let the model run for a long time but stop much before when the performance on the held out set starts to degrade.\n",
    "\n",
    "However we also do not want to model to overfit on the held out set and limit the generalization capabilities of the model. Hence we should create both a validation set and a test set to verify the generalization capability of the model, and to make sure that we dont over fit and under fit on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "90109072-2b90-4138-8453-5697a1f9ff99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING - All triples will be processed in the same batch (batches_count=1). When processing large graphs it is recommended to batch the input knowledge graph instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average TransE Loss:   0.014648 — Best validation (mrr): 0.095652:   8%|▌     | 849/10000 [14:14<2:33:35,  1.01s/epoch]\n"
     ]
    }
   ],
   "source": [
    "early_stopping_params = { 'x_valid': X_valid,   # Validation set on which early stopping will be performed\n",
    "                          'criteria': 'mrr',    # metric to watch during early stopping\n",
    "                          'burn_in': 150,       # Burn in time, i.e. early stopping checks will not be performed till 150 epochs\n",
    "                          'check_interval': 50, # After burn in time, early stopping checks will be performed at every 50th epochs (i.e. 150, 200, 250, ...)\n",
    "                          'stop_interval': 2,   # If the monitored criteria degrades for these many epochs, the training stops. \n",
    "                          'corrupt_side':'s,o'  # Which sides to corrupt furing early stopping evaluation (default both subject and obj as described earlier)\n",
    "                        }\n",
    "\n",
    "# Create a TransE model as earlier -\n",
    "\n",
    "model = TransE(k = 100,\n",
    "              epochs = 10000,\n",
    "              eta = 1,\n",
    "              loss = 'multiclass_nll',\n",
    "              initializer = 'xavier', initializer_params = {'uniform': False},\n",
    "              regularizer = 'LP', regularizer_params = {'lambda': 0.0001, 'p': 3},\n",
    "              optimizer = 'adam', optimizer_params = {'lr': 0.001},\n",
    "              seed = 0, batches_count = 1, verbose = True)\n",
    "\n",
    "model.fit(X_train, \n",
    "         early_stopping = True,\n",
    "         early_stopping_params = early_stopping_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "eed2c344-6d62-4a41-87d6-3012dfda1948",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model, 'TransE-early-stopping.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d1c95f71-37d7-465d-8f53-49419bf51ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:22<00:00, 45.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Rank: 506.239\n",
      "Mean Reciprocal Rank: 0.16526740494343775\n",
      "Hits@1: 0.101\n",
      "Hits@10: 0.288\n",
      "Hits@100: 0.5945\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model with filter\n",
    "X_filter = np.concatenate([X_train, X_valid, X_test], 0)\n",
    "ranks = evaluate_performance(X_test, \n",
    "                             model=model, \n",
    "                             filter_triples=X_filter)\n",
    "# display the metrics\n",
    "display_aggregate_metrics(ranks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fefd34-dc87-488d-a273-a67e1a756cee",
   "metadata": {},
   "source": [
    "# Practical Evaluation Protocols\n",
    "\n",
    "These standard protocols with the KG with millions of entities are not feasible. So, there are some practical methods -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7ea65c-8d2c-4d12-99d4-34111baf9f92",
   "metadata": {},
   "source": [
    "### Evaluating by corrupting specific sides -\n",
    "\n",
    "Say our test set is made up of triples of type <movie, film_language, language_category> and we want to find if our model can correctly find the language of the movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "27114766-6882-4551-a73a-084c1fa6ed75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['the mummy returns', '/film/film/language', 'english language'],\n",
       "       ['love affair', '/film/film/language', 'english language'],\n",
       "       ['vicky cristina barcelona', '/film/film/language',\n",
       "        'spanish language'],\n",
       "       ['borat', '/film/film/language', 'english language'],\n",
       "       ['/m/03gyvwg', '/film/film/language', 'japanese language'],\n",
       "       ['and the band played on', '/film/film/language',\n",
       "        'english language'],\n",
       "       ['/m/09gb_4p', '/film/film/language', 'english language'],\n",
       "       ['from russia with love', '/film/film/language',\n",
       "        'turkish language'],\n",
       "       ['titanic', '/film/film/language', 'italian language'],\n",
       "       ['salt', '/film/film/language', 'english language'],\n",
       "       ['superman ii', '/film/film/language', 'english language'],\n",
       "       ['spy game', '/film/film/language', 'english language']],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_movie_languages = X_test[X_test[:, 1] == '/film/film/language']\n",
    "X_test_movie_languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "89203c19-3564-4817-8d44-e0282ab9cd2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 12/12 [00:00<00:00, 25.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Rank: 375.9166666666667\n",
      "Mean Reciprocal Rank: 0.2604060555937539\n",
      "Hits@1: 0.16666666666666666\n",
      "Hits@10: 0.4166666666666667\n",
      "Hits@100: 0.6666666666666666\n",
      "\n",
      "Size of test set: (12, 3)\n",
      "Size of ranks: (12, 2)\n"
     ]
    }
   ],
   "source": [
    "ranks = evaluate_performance(X_test_movie_languages, model = model, filter_triples = X_filter)\n",
    "display_aggregate_metrics(ranks)\n",
    "print('\\nSize of test set:', X_test_movie_languages.shape)\n",
    "print('Size of ranks:', ranks.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f60ba88-2a8d-47a4-9eb2-ef5c62962d1b",
   "metadata": {},
   "source": [
    "Some types of corruption - \n",
    "\n",
    "* s for subject corruption only\n",
    "* o for object corruption only\n",
    "* s+o for subject and object corruption. Returns a single rank.\n",
    "* s,o for subject and object corruption separately (default). Returns 2 ranks. This is equivalent to calling evaluate_performance twice with s and o.\n",
    "\n",
    "As here we want to find if the model can correctly find the language of the movie, we will corrupt the object side of the thing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f97378b6-40e1-455f-bb9f-52d6ef6bed90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 12/12 [00:00<00:00, 36.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Rank: 220.33333333333334\n",
      "Mean Reciprocal Rank: 0.34339267265349743\n",
      "Hits@1: 0.25\n",
      "Hits@10: 0.5\n",
      "Hits@100: 0.9166666666666666\n",
      "\n",
      "Size of test set: (12, 3)\n",
      "Size of ranks: (12,)\n"
     ]
    }
   ],
   "source": [
    "ranks = evaluate_performance(X_test_movie_languages, \n",
    "                            model = model,\n",
    "                            filter_triples = X_filter,\n",
    "                            corrupt_side = 'o')\n",
    "\n",
    "display_aggregate_metrics(ranks)\n",
    "print('\\nSize of test set:', X_test_movie_languages.shape)\n",
    "print('Size of ranks:', ranks.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0699d4ff-0c7b-428b-80ce-60b5243e7212",
   "metadata": {},
   "source": [
    "### Evaluating against a subset of entities\n",
    "\n",
    "Depending on the use case or size of the graph, you may want to evaluate the test set by generating corruptions only from a subset of entities.\n",
    "For example, let's say we are doing a genetic study using KG. The graph may have different entity types like patient, diseases, genes, mutations, co-morbidities,ect. Say we want to find out what mutations cause disease i.e. **< ?, causes, disease_name>**. For this use case it doesnt make sense to replace the placeholder with all the entities. A logical replacement would be by using all the mutations.\n",
    "\n",
    "Similarly for our use case, we are interested in finding the language of the movie. So it makes sense to use only language categories to generate the corruptions for the object side. It also makes the task easier for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "83d4d7ce-bd45-4035-a0cb-38ea56e296eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of corruptions generated per triple is: 14184\n"
     ]
    }
   ],
   "source": [
    "print('The number of corruptions generated per test triple is:', len(model.ent_to_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a4af2631-bd5c-46ad-9da8-a773b6d411ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of languages in KG: 61\n",
      "\n",
      " {'irish', 'filipino language', 'french', 'thai language', 'yue chinese', 'bengali language', 'france', 'mandarin chinese', 'indonesian language', 'turkish language', 'tamil language', 'finnish language', 'japanese language', 'tagalog language', 'latin language', 'serbo-croatian language', 'standard chinese', 'polish language', 'german language', 'persian language', 'punjabi language', 'vietnamese language', 'chinese language', 'spanish language', 'croatian language', 'korean language', 'hebrew language', 'dutch language', 'greek language', 'cantonese', 'yiddish language', 'czech language', 'russian language', 'germany', 'ukrainian language', 'afrikaans language', 'malay language', 'italian language', 'arabic language', 'sinhala language', 'danish language', 'bulgarian language', 'albanian language', 'swedish language', 'hindi language', 'swahili language', 'italian food', 'serbian language', 'romanian language', 'norwegian language', 'urdu language', 'khmer language', 'welsh language', 'silent film', 'german food', 'hungarian language', 'english language', 'american english', 'england', 'gujarati language', 'portuguese language'}\n"
     ]
    }
   ],
   "source": [
    "unique_languages = set(X_train[X_train[:, 1] == '/film/film/language'][:, 2])\n",
    "\n",
    "print('Number of languages in KG:', len(unique_languages))\n",
    "print('\\n', unique_languages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca26b88-39f2-4f69-ac21-e9e2e93265e6",
   "metadata": {},
   "source": [
    "Now let's create the corruptions only on these languages using entities_subset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "312e9094-44d9-4487-a395-0deeff2ab9b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\github_repos\\ml-engineering\\graph_models\\ampligraph_env\\lib\\site-packages\\ampligraph\\latent_features\\models\\EmbeddingModel.py:1329: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if corruption_entities == 'all':\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 12/12 [00:00<00:00, 24.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Rank: 13.083333333333334\n",
      "Mean Reciprocal Rank: 0.3456077061340219\n",
      "Hits@1: 0.25\n",
      "Hits@10: 0.5\n",
      "Hits@100: 1.0\n"
     ]
    }
   ],
   "source": [
    "ranks = evaluate_performance(X_test_movie_languages,\n",
    "                            model = model,\n",
    "                            filter_triples = X_filter,\n",
    "                            corrupt_side = 'o',\n",
    "                            entities_subset = list(unique_languages))\n",
    "display_aggregate_metrics(ranks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad935d05-c426-4bfd-93d9-383eb7dd9fdb",
   "metadata": {},
   "source": [
    "Usually, we can see a drastic increase in the metric values mainly because we are using fewer semantically \"valid\" corruptions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ampligraph_env",
   "language": "python",
   "name": "ampligraph_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
